# Credit Default Risk  
## Overview
A project to select a machine learning model that can predict the risk of credit default using the [Kaggle Credit Risk Dataset](https://www.kaggle.com/datasets/laotse/credit-risk-dataset).  In this project, I performed data cleaning and validation, used statistical and mathematical methods to detect data leakage, encoded and transformed features, and employed several machine learning algorithms (Support Vector Machines, K-Nearest Neighbors, Linear Regression, and Random Forest Classifier).  
**Context:** Many of the methods I used are employed in business contexts to find a person's or a business's default risk and help a lender or creditor determine the loan terms. These methods are also applicable to many other risk-related industries, such as insurance, hiring/firing employees, or investing. I also attempted to showcase my ability to engineer features, detect data leakage, and use my knowledge of statistics to find the best features.  
**The Dataset:** This dataset was posted by Kaggle user [Lao Tse](https://www.kaggle.com/laotse) and posted under a public domain license. The dataset contains synthetic data that was generated by Lao Tse (presumably).  

## Tools Used:  
### Python was used as the primary coding language.  
**Visualization:** Matplotlib and Seaborn  
**Data Storage/Structures:** Pandas and NumPy  
**Data Transformation and Feature Selection:** sklearn, scipy  

## Methods:  
**Data Visualization:** I plotted data using bar charts, kernel density plots, and heatmaps to check for distributions, data leakage, and correlation to the target.  
**Exploratory Data Analysis:** I checked for unrealistic outliers or missing values and considered whether they should be removed or imputed. I also checked for data leakage using t-tests and correlation coefficients. Finally, I checked for class imbalances that could affect a machine learning algorithm's ability to generalize to a new data set.  
**Feature Engineering:** I performed simple mathematical transforms and encoding to reveal insights and look for data leakage.  
**Feature Selection:** I used SVC and Random Forest Classifiers to determine which feature sets provided the best predictive power.  
**Model Selection:** I used grid searching algorithms to scan through the parameter space and find the best model. Many models performed approximately the same, so I made a decision based on run time, computational resources, costs, and implementation difficulty.  

## Results:  
I found that a **Random Forest Classifier** provided the best results with the least effort and complexity. The parameters I found for the best model were:  
**criterion:** entropy  
**max_features:** None  
**n_estimators:** 200  
**class_weights:** random_subsample  
**Others:** Defaults
